{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting promotion sales 1 week ahead\n",
    "\n",
    "## Explanation of the model\n",
    "* The model forecasts the Unit Sales one week ahead in case of a promotion or no-promotion on an article level. \n",
    "* The anonymized parquet dataset used as training data contains daily sales of various AH products in 2016 and 2017\n",
    "\n",
    "\n",
    "## Explanation of the data: \n",
    "* DateKey: date identifier\n",
    "* StoreCount: number of stores the article is available at\n",
    "* ShelfCapacity: total capacity of shelfs over all stores\n",
    "* PromoShelfCapacity: additional ShelfCapacity during promotion \n",
    "* IsPromo: indicator if article is in promotion \n",
    "* ItemNumber: item identification number\n",
    "* CategoryCode: catergory identification number (product hierarchy) \n",
    "* GroupCode: group identification number (product hierarchy) \n",
    "* UnitSales: number of consumer units sold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"dataset.csv\"\n",
    "\n",
    "df_prep = pd.read_csv(path, sep=\";\", header=0)\n",
    "df_prep.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transform the target column!\n",
    "df_prep['UnitSales'] = np.log(df_prep['UnitSales']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the column DateKey (string) to a date column\n",
    "df_prep['DateKey'] = pd.to_datetime(df_prep['DateKey'], format='%Y%m%d')\n",
    "df_prep['month'] = df_prep['DateKey'].dt.month\n",
    "df_prep['weekday'] = df_prep['DateKey'].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null values\n",
    "df_prep_clean_0 = df_prep[df_prep['UnitSales'].notnull()].copy()\n",
    "df_prep_clean = df_prep_clean_0[df_prep_clean_0['ShelfCapacity'].notnull()].copy()\n",
    "\n",
    "# Convert columns to correct format\n",
    "df_prep_clean['month'] = df_prep_clean['month'].astype('category')\n",
    "df_prep_clean['weekday'] = df_prep_clean['weekday'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train and test sets\n",
    "Given that we are working with time series data in the sense that there is obvious temporal relations in the data, it is crucial to ensure that when training the model, no information about the future is present.\n",
    "\n",
    "We split in a temporal sensible way such that the test set is in the future and could not have been used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_split = df_prep_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the data in a train set and a test set, we do this, 80, 20 percent respectively.\n",
    "nr_of_unique_dates = len(df_to_split.DateKey.unique())\n",
    "train_split_delta = round(nr_of_unique_dates * 0.8)\n",
    "train_split_date = df_to_split.DateKey.dt.date.min() + datetime.timedelta(days=train_split_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(total_df, tr_split_date):\n",
    "    tr_df = total_df[total_df['DateKey'].dt.date <= tr_split_date].copy()\n",
    "    tst_df = total_df[total_df['DateKey'].dt.date > tr_split_date].copy()\n",
    "    return tr_df, tst_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df_to_split, train_split_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make categories out of the following columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['GroupCode'] = train_df['GroupCode'].astype('category')\n",
    "train_df['ItemNumber'] = train_df['ItemNumber'].astype('category')\n",
    "train_df['CategoryCode'] = train_df['CategoryCode'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out items that were not present in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine unique item numbers, and filter the validation and test on these\n",
    "items_we_train_on = train_df['ItemNumber'].unique()\n",
    "test_df_filtered = test_df[test_df['ItemNumber'].isin(items_we_train_on)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_filtered['GroupCode'] = test_df_filtered['GroupCode'].astype('category')\n",
    "test_df_filtered['ItemNumber'] = test_df_filtered['ItemNumber'].astype('category')\n",
    "test_df_filtered['CategoryCode'] = test_df_filtered['CategoryCode'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage the split has been succesful. We will use the training dataframe to train the model. We use the test dataframe to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataframe where label and features are included in an appropriate way and add lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lagged_feature_to_df(input_df, lag_iterator, feature):\n",
    "    \"\"\"\n",
    "    A function that will expand an input dataframe with lagged variables of a specified feature\n",
    "    Note that the lag is calculated over time (datekey) but also kept appropriate over itemnumber (article)\n",
    "    \n",
    "    input_df: input dataframe that should contain the feature and itemnr\n",
    "    lag_iterator: an object that can be iterator over, that includes info about the requested nr of lags\n",
    "    feature: feature that we want to include the lag of in the dataset\n",
    "    \"\"\"\n",
    "    output_df = input_df.copy()\n",
    "    for lag in lag_iterator:\n",
    "        df_to_lag = input_df[['DateKey', 'ItemNumber', feature]].copy()\n",
    "        # we add the nr of days equal to the lag we want\n",
    "        df_to_lag['DateKey'] = df_to_lag['DateKey'] + datetime.timedelta(days=lag)\n",
    "        \n",
    "        # the resulting dataframe contains sales data that is lag days old for the date that is in that row\n",
    "        df_to_lag = df_to_lag.rename(columns={feature: feature+'_-'+str(lag)})\n",
    "        \n",
    "        # we join this dataframe on the original dataframe to add the lagged variable as feature\n",
    "        output_df = output_df.merge(df_to_lag, how='left', on=['DateKey', 'ItemNumber'])\n",
    "    # drop na rows that have been caused by these lags\n",
    "    return output_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_of_lags = [7, 14, 21] # 1 week ago, 2 weeks ago, 3 weeks ago\n",
    "feature_to_lag = 'UnitSales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the lags per dataset (no data leakage) and also do the NaN filtering per set\n",
    "train_df_lag = add_lagged_feature_to_df(train_df, range_of_lags, feature_to_lag)\n",
    "test_df_lag = add_lagged_feature_to_df(test_df_filtered, range_of_lags, feature_to_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_lag.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Datekey\n",
    "This was used for the lag construction, but will not be used in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_lag_clean = train_df_lag.drop(columns=['DateKey'])\n",
    "test_df_lag_clean = test_df_lag.drop(columns=['DateKey'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_lag_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert the data in the required format for the model (label y and features x)\n",
    "train_y, train_X = train_df_lag_clean['UnitSales'], train_df_lag_clean.drop(columns=['UnitSales'])\n",
    "test_y, test_X = test_df_lag_clean['UnitSales'], test_df_lag_clean.drop(columns=['UnitSales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: [RandomForest Regression](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model settings\n",
    "rfr = RandomForestRegressor(\n",
    "    n_estimators=100, max_features=round(len(train_X.columns)/3), max_depth=len(train_X.columns),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model. Takes a couple of minutes.\n",
    "rf_model = rfr.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "rf_y_pred = rf_model.predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate RMSE of the log unit sales for the rf model\n",
    "mean_squared_error(rf_y_pred, test_y, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the MAE\n",
    "mean_absolute_error(rf_y_pred, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of making a prediction for new inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_log_to_units(prediction):\n",
    "    return int(math.exp(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['StoreCount', 'ShelfCapacity', 'PromoShelfCapacity', 'IsPromo',\n",
    "       'ItemNumber', 'CategoryCode', 'GroupCode', 'month', 'weekday',\n",
    "       'UnitSales_-7', 'UnitSales_-14', 'UnitSales_-21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_example = pd.DataFrame(\n",
    "    data=[\n",
    "        (781, 12602.000, 4922, True, 8646, 7292, 5494, 11, 3, 6.190, 6.217, 6.075),\n",
    "    ], \n",
    "    columns=columns,\n",
    ")\n",
    "custom_example_y = 8.187021067343505"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_pred = rf_model.predict(custom_example)\n",
    "print(f'Model prediction: {example_pred[0]} which means a predicted UnitSales of {convert_log_to_units(example_pred[0])}')\n",
    "print(f'Real value is: {custom_example_y} which means a predicted UnitSales of {convert_log_to_units(custom_example_y)}')\n",
    "print(f'So the delta is {abs(convert_log_to_units(custom_example_y) - convert_log_to_units(example_pred[0]))} units')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_example_pred = rf_model.predict([[781, 12602.000, 4922, True, 8646, 7292, 5494, 11, 3, 6.190, 6.217, 6.075]])\n",
    "convert_log_to_units(another_example_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk\n",
    "filename = 'forecasting_model.pkl'\n",
    "pickle.dump(rf_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "loaded_from_disk_pred = loaded_model.predict([[781, 12602.000, 4922, True, 8646, 7292, 5494, 11, 3, 6.190, 6.217, 6.075]])\n",
    "convert_log_to_units(loaded_from_disk_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
